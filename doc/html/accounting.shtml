<!--#include virtual="header.txt"-->

<h1>Accounting</h1>
<p>SLURM collects accounting information for every job and job step 
executed. 
Information is available about both currently executing jobs and 
jobs which have already terminated and can be viewed using the 
<b>sacct</b> command. 
Resource usage is reported for each task and this can be useful to 
detect load imbalance between the tasks. 
SLURM version 1.2 and earlier supported the storage of accounting 
records to a text file.
Beginning in SLURM version 1.3 accounting records can be written to 
a database. </p>

<p>There are three distinct plugin types associated with resource accounting.
The configuration parameters associated with these plugins include:
</p><ul>
<li><b>JobCompType</b> controls how job completion information is 
recorded. This can be used to record basic job information such
as job name, user name, allocated nodes, start time, completion 
time, exit status, etc. If the preservation of only basic job 
information is required, this plugin should satisfy your needs
with minimal overhead. You can store this information in a 
text file, <a href="http://www.mysql.com/">MySQL</a> or 
<a href="http://www.postgresql.org/">PostgreSQL</a> 
database optionally using either 
<a href="http://www.clusterresources.com/pages/products/gold-allocation-manager.php">Gold</a>
or SlurmDBD for added database security.</li>
<li><b>JobAcctGatherType</b> is operating system dependent and 
controls what mechanisms are used to collect accounting information.
Supported values are <i>jobacct_gather/aix</i>, <i>jobacct_gather/linux</i>
and <i>jobacct_gather/none</i> (no information collected).</li>
<li><b>AccountingStorageType</b> controls how detailed job and job 
step information is recorded. You can store this information in a 
text file, <a href="http://www.mysql.com/">MySQL</a> or 
<a href="http://www.postgresql.org/">PostgreSQL</a> 
database optionally using either 
<a href="http://www.clusterresources.com/pages/products/gold-allocation-manager.php">Gold</a>
or SlurmDBD for added security.</li>
</ul>

<p>Storing the information into text files is very simple. 
Just configure the appropriate plugin (e.g. 
<i>AccountingStorageType=accounting_storage/filetxt</i> and/or 
<i>JobCompType=jobcomp/filetxt</i>) and then specify the 
pathname of the file (e.g.
<i>AccountingStorageLoc=/var/log/slurm/accounting</i> and/or 
<i>JobCompLoc=/var/log/slurm/job_completions</i>).
Use the <i>logrotate</i> or similar tool to prevent the 
log files from getting too large.
Send a SIGHUP signal to the <i>slurmctld</i> deaemon 
after moving the files, but before compressing them so
that new log files will be created.</p>

<p>Storing the data directly into a database from SLURM may seem 
attractive, but that requires the availability of user name and 
password data not only for the SLURM control daemon (slurmctld), 
but also user commands which need to access the data (sacct and
sacctmgr). 
Making information available to all users makes database security 
more difficult to provide, sending the data through an intermediate
daemon can provide better security. 
Gold and SlurmDBD are two such services. 
Our initial implementation relied upon Gold, but we found its
performance to be inadequate for our needs and developed SlurmDBD.
SlurmDBD (SLURM Database Daemon) is written in C, multi-threaded, 
secure, and considerably faster than Gold.
The configuration required to use SlurmDBD will be described below.
Direct database or Gold use would be similar.</p>

<p>Note that SlurmDBD relies upon existing SLURM plugins
for authentication and database use, but the other SLURM 
commands and daemons are not required on the host where
SlurmDBD is installed. Install the <i>slurmdbd</i> and 
<i>slurm-plugins</i> RPMs on the computer when SlurmDBD
is to execute.</p>

<h2>Infrastructure</h2>

<p>If the SlurmDBD is executed on a different cluster than the 
one managed by SLURM, possibly to collect data from multiple 
clusters in a single location, there are some constraints on 
the user space.
The user ID associated with <i>SlurmUser</i> must be uniform 
across all clusters. 
Accounting is maintained by user name (not user ID), but a
given user name should refer to the same person across all 
of the computers.</p>

<p>The best way to insure security of the data is by authenticating 
communications to the SlurmDBD and we recommend 
<a href="http://home.gna.org/munge/">Munge</a> for that purpose.
Munge was designed to support authentication within a cluster.
If you have one cluster managed by SLURM and execute the SlurmDBD 
on that one cluster, the normal Munge configuration will suffice.
Otherwise Munge should then be installed on all nodes of all 
SLURM managed clusters plus the machine where SlurmDBD executes.
You then have a choice of either having a single Munge key for 
all of these computers or maintaining a unique key for each of the 
clusters plus a second key for communications between the clusters
for better security.
Munge enhancements are planned to support two keys within a single 
configuration file, but presently two different daemons must be 
started with different configuration files to support two different 
keys. 
If a Munge separate daemon configured to provide enterprise-wide 
authentication, it will have a unique named pipe configured for 
communications. 
The pathname of this pipe will be needed in the SLURM and SlurmDBD
configuration files (slurm.conf and slurmdbd.conf respectively, 
more details are provided below).</p>

<h2>SLURM Configuration</h2>

<p>Several SLURM configuration parameters must be set to support
archiving information in SlurmDBD. SlurmDBD has a separate configuration
file which is documented in a separate section.
Note that you can write accounting information to SlurmDBD
while job completion records are written to a text file or 
not maintained at all. 
If you don't set the configuration parameters that begin 
with "JobComp" then job completion records will not be recorded.</p>

<ul>
<li><b>AccountingStorageEnforce</b>:
If you want to prevent users from running jobs if their <i>association</i>
(a combination of cluster, account, and user names. For more
flexibility in accounting the association can also include a partition
name, but it is not necissary.) is not in the database, then set this
to "1". Otherwise jobs will be executed based upon policies configured
in SLURM on each cluster. </li>

<li><b>AccountingStorageHost</b>: The name or address of the host where SlurmDBD executes
      </li>

<li><b>AccountingStoragePass</b>: If using SlurmDBD with a second Munge
daemon, store the pathname of the named socket used by Munge to provide
enterprise-wide. Otherwise the default Munge daemon will be used. . </li>

<li><b>AccountingStoragePort</b>:
The network port that SlurmDBD accepts communication on.</li>

<li><b>AccountingStorageType</b>:
Set to "accounting_storage/slurmdbd".</li>

<li><b>ClusterName</b>:
Set to a unique name for each Slurm-managed cluster so that 
accounting records from each can be identified.</li>

<li><b>JobCompHost</b>:
The name or address of the host where SlurmDBD executes.</li>

<li><b>JobCompPass</b>:
If using SlurmDBD with a second Munge daemon, store the pathname of 
the named socket used by Munge to provide enterprise-wide.
Otherwise the default Munge daemon will be used.</li>

<li><b>JobCompPort</b>:
The network port that SlurmDBD accepts communication on.</li>

<li><b>JobCompType</b>:
Set to "jobcomp/slurmdbd".</li>
</ul>

<h2>SlurmDBD Configuration</h2>

<p>SlurmDBD requires its own configuration file called "slurmdbd.conf". 
This file should be only on the computer where SlurmDBD executes and 
should only be readable by the user which executes SlurmDBD (e.g. "slurm").
This file should be protected from unauthorized access since it
contains a database login name and password.
See "man slurmdbd.conf" for a more complete description of the 
configuration parameters. 
Some of the more important parameters include:</p>

<ul>
<li><b>AuthInfo</b>:
If using SlurmDBD with a second Munge daemon, store the pathname of 
the named socket used by Munge to provide enterprise-wide.
Otherwise the default Munge daemon will be used.</li>

<li><b>AuthType</b>:
Define the authentication method for communications between SLURM 
components. A value of "auth/munge" is recommended.</li>

<li><b>DbdHost</b>:
The name of the machine where the Slurm Database Daemon is executed. 
This should be a node name without the full domain name (e.g. "lx0001"). 
This value must be specified.</li>

<li><b>DbdPort</b>:
The port number that the Slurm Database Daemon (slurmdbd) listens 
to for work. The default value is SLURMDBD_PORT as established at system 
build time. If none is explicitly specified, it will be set to 6819.
This value must be equal to the <i>SlurmDbdPort</i> parameter in the
slurm.conf file.</li>

<li><b>LogFile</b>:
Fully qualified pathname of a file into which the Slurm Database Daemon's 
logs are written.
The default value is none (performs logging via syslog).</li>

<li><b>PluginDir</b>:
Identifies the places in which to look for SLURM plugins. 
This is a colon-separated list of directories, like the PATH 
environment variable. 
The default value is "/usr/local/lib/slurm".</li>

<li><b>SlurmUser</b>:
The name of the user that the <i>slurmctld</i> daemon executes as. 
This user must exist on the machine executing the Slurm Database Daemon
and have the same user ID as the hosts on which <i>slurmctld</i> execute.
For security purposes, a user other than "root" is recommended.
The default value is "root". </li>

<li><b>StorageHost</b>:
Define the name of the host the database is running where we are going
to store the data.
Ideally this should be the host on which slurmdbd executes.</li>

<li><b>StorageLoc</b>:
Specifies the name of the database where accounting 
records are written, for databases the default database is
slurm_acct_db.  Note the name can not have a '/' in it or the
default will be used.</li>

<li><b>StoragePass</b>:
Define the password used to gain access to the database to store 
the job accounting data.</li>

<li><b>StoragePort</b>:
Define the port on which the database is listening.</li>

<li><b>StorageType</b>:
Define the accounting storage mechanism type.
Acceptable values at present include 
"accounting_storage/gold", "accounting_storage/mysql", and
"accounting_storage/pgsql".
The value "accounting_storage/gold" indicates that account records
will be written to Gold, which maintains its own database.
Use of Gold is not recommended due to reduced performance without 
providing any additional security.
The value "accounting_storage/mysql" indicates that accounting records
should be written to a MySQL database specified by the 
<i>StorageLoc</i> parameter.
The value "accounting_storage/pgsql" indicates that accounting records
should be written to a PostgreSQL database specified by the 
<i>StorageLoc</i> parameter.
This value must be specified.</li>

<li><b>StorageUser</b>:
Define the name of the user we are going to connect to the database
with to store the job accounting data.</li>
</ul>

<h2>Tools</h2>

<p>There are two tools available to work with accounting data,
<b>sacct</b> and <b>sacctmgr</b>. 
Both of these tools will get or set data through the SlurmDBD daemon. 
Sacct is used to generate accounting report for both running and 
completed jobs.
Sacctmgr is used to manage associations in the database: 
add or remove clusters, add or remove users, etc.
See the man pages for each command for more information.</p>

<p>Web interfaces with graphical output is currently under
development and should be available in the summer of 2008.
A tool to report node state information is also under development.</p>

<h2>Database Configuration</h2>

<p>Accounting records are maintained based upon what we refer 
to as an <i>Association</i>,
which consists of four elements: cluster, account, and user names. For
more flexibility in accounting the association can also include a
partition name, but it is not necessary. Use the <i>sacctmgr</i>
command to create and manage these records. There is an order to set up
accounting associations. You must define clusters before you add
accounts and you must add accounts before you can add users. </p>

<p>For example, to add a cluster named "snowflake" to the database
execute this line:</p>
<pre>
sacctmgr add cluster snowflake
</pre>

<p>Add accounts "none" and "test" to cluster "snowflake" with an execute 
line of this sort:</p>
<pre>
sacctmgr add account none,test Cluster=snowflake \
  Description="none" Organization="none" 
</pre>

<p>If you have more clusters you want to add these accounts to you
can either not specify a cluster, which will add the accounts to all
clusters in the system, or comma separate the cluster names you want
to add to in the cluster option.
Note that multiple accounts can be added at the same time 
by comma separating the names. 
Some Description of the account and the organization which it belongs
must be specified. 
These terms can be used later to generated accounting reports.
Accounts may be arranged in a hierarchical fashion, for example accounts 
<i>chemistry</i> and <i>physics</i> may be children of the account <i>science</i>. 
The hierarchy may have an arbitrary depth. 
To do this one only needs to specify the <i>parent='' </i>option in the add 
account line.
For the example above execute</p>
<pre>
sacctmgr add account science \
   Description="science accounts" Organization=science
sacctmgr add account chemistry,physics parent=science \
   Description="physical sciences" Organization=science
</pre>

<p>Add users to accounts using similar syntax.
For example, to permit user <i>da</i> to execute jobs on all clusters
with a default account of <i>test</i> execute:</p>
<pre>
sacctmgr add user da default=test
</pre>
<p>If <b>AccountingStorageEnforce=1</b> is configured in the slurm.conf of 
the cluster <i>snowflake</i> then user <i>da</i> would be
allowed to run in account <i>test</i> and any other accounts added
in the future.
Any attempt to use other accounts will result in the job being 
aborted. 
Account <i>test</i> will be the default if he doesn't specify one in a 
srun line.</p>

<p>Partition names can also be added to an "add user" command with the
Partition='partitionname' option to specify an association specific to
a slurm partition.</p>

<!-- For future use
<h2>Cluster Options</h2>

<p>When either adding or modifying a cluster, these are the options 
available with sacctmgr:
<ul>
<li><b>Name=</b> Cluster name</li>

<li><b>Fairshare=</b> Used for determining priority</li>

<li><b>MaxJobs=</b> Limit number of jobs a user can run in this account</li>

<li><b>MaxNodes=</b>Limit number of nodes a user can allocate in this 
account</li>

<li><b>MaxWall=</b>Limit wall clock time a job can run</li>

<li><b>MaxCPUSecs=</b> Limit cpu seconds a job can run</li>
</ul>
!-->

<h2>Account Options</h2>

<p>When either adding or modifying an account, the following sacctmgr 
options are available:
<ul>
<li><b>Description=</b> Description of the account. (Required on creation)</li>

<li><b>Organization=</b>Organization of the account. (Required on creation)</li>

<li><b>Name=</b> Name of account</li>

<li><b>Cluster=</b> Only add this account to these clusters.
The account is added to all defined clusters by default.</li>

<li><b>Parent=</b> Make this account a child of this other account.</li>

<!-- For future use
<li><b>QOS=</b> Quality of Service</li>

<li><b>Fairshare=</b> Used for determining priority</li>

<li><b>MaxJobs=</b> Limit number of jobs a user can run in this account</li>

<li><b>MaxNodes=</b>Limit number of nodes a user can allocate in this account</li>

<li><b>MaxWall=</b>Limit wall time a job can run</li>

<li><b>MaxCPUSecs=</b> Limit cpu seconds a job can run</li>
!-->
</ul>

<h2>User Options</h2>

<p>When either adding or modifying a user, the following sacctmgr 
options are available:

<ul>
<li><b>Name=</b> User name</li>

<li><b>DefaultAccount=</b> Default account for the user, used when no account 
is specified when a job is sumbitted. (Required on creation)</li>

<li><b>AdminLevel=</b> This field is used to allow a user to add accounting 
privileges to this user. Valid options are 
<ul>
<li>None</li>
<li>Operator: can add, modify,and remove users, and add other operators)</li>
<li>Admin: In addition to operator privileges these users can add, modify, 
and remove accounts and clusters</li>
</ul>

<li><b>Account=</b> Account(s) to add user to</li>

<li><b>Cluster=</b> Only add to accounts on these clusters (default is all clusters)</li>

<li><b>Partition=</b> Name of Slurm partition this association applies to</li>

<!-- For future use
<li><b>QOS=</b> Quality of Service</li>

<li><b>Fairshare=</b> Used for determining priority</li>

<li><b>MaxJobs=</b> Limit number of jobs a user can run in this account</li>

<li><b>MaxNodes=</b> Limit number of nodes a user can allocate in this account</li>

<li><b>MaxWall=</b> Limit wall time a job can run</li>

<li><b>MaxCPUSecs=</b> Limit cpu seconds a job can run</li>
!-->
</ul>

<!-- For future use
<h2>Limit enforcement</h2>

<p>When limits are developed they will work in this order...
If a user has a limit set SLURM will read in those, 
if not we will refer to the account associated with the job. 
If the account doesn't have the limit set we will refer to 
the cluster's limits. 
If the cluster doesn't have the limit set no limit will be enforced.
!-->

<h2>Modifying Entities</h2>

<p>When modifying entities, you can specify many different options in 
SQL-like fashion, using key words like <i>where</i> and <i>set</i>.
A typical execute line has the following form:
<pre>
sacctmgr modify &lt;entity&gt; set &lt;options&gt; where &lt;options&gt;
</pre>

<p>For example:</p>
<pre>
sacctmgr modify user set default=none where default=test
</pre>
<p>will change all users with a default account of "test" to account "none".
Once an entity has been added, modified or removed, the change is 
sent to the appropriate SLURM daemons and will be available for use 
instantly.</p>

<h2>Removing Entities</h2>

<p>Removing entities using an execute line similar to the modify example above,
but without the set options.
For example, remove all users with a default account "test" using the following 
execute line:</p>
<pre>
sacctmgr remove user where default=test
</pre>

<h2>Node State Information</h2>

<p>Node state information is also recorded in the database. 
Whenever a node goes DOWN or becomes DRAINED that event is 
logged along with the node's <i>Reason</i> field. 
This can be used to generate various reports.

<p style="text-align: center;">Last modified 25 March 2008</p>

</ul></body></html>
