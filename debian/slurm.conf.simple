############################################################################### 
#                 Sample configuration file for SLURM 1.3
###############################################################################
#
# This file holds the system-wide SLURM configuration. It is read
# by SLURM clients, daemons, and the SLURM API to determine where
# and how to contact the SLURM controller, what other nodes reside
# in the current cluster, and various other configuration information.
#
# SLURM configuration parameters take the form Keyword=Value, where
# at this time, no spacing is allowed to surround the equals (=) sign.
# Many of the config values are not mandatory, and so may be left
# out of the config file. We will attempt to list the default 
# values for those parameters in this file.
#
# This simple configuration provides a control machine named "laptop"
# to run the Slurm's central management daemon and a single node
# named "server" which execute jobs. Both machine should have Slurm
# installed and use this configuration file. If you have a similar
# configuration just change the values of ControlMachine, for the
# control machine and PartitionName and NodeName for job execution
#
###############################################################################

#  
#     SLURM daemon configuration
#     ========================================================================

#
# o Define the location of the SLURM controller and backup controller:
#    "ControlMachine"   : hostname of the primary controller
#    "ControlAddr"      : hostname used to contact the primary controller
#    "BackupController" : hostname of the backup controller 
#    "BackupAddr"       : hostname used to contact backup controller
#
# Example:
#
# ControlMachine=dev0
# ControlAddr=edev0		# default: same as ControlMachine
# BackupController=dev1		# default: no backup controller
# BackupAddr=edev1		# default: same as BackupController

ControlMachine=laptop

#
# o Define the SLURM controller "save state" directory
#
#   The SLURM controller, slurmctld, will periodically save state
#   into this directory so that said state may be recovered after
#   a fatal system error. For best results when using a backup 
#   controller, the filesystem on which this directory resides 
#   should be shared between the "ControlMachine" and "BackupController"
#
# Example:
# 
# StateSaveLocation=/mnt/slurm	# default: "/tmp"

StateSaveLocation=/var/run/slurm-llnl/slurmctld

#
# o Define the slurmd "save state" directory
#
#   The SLURM daemon executing on each host, slurmd, will periodically 
#   save state into this directory so that said state may be recovered
#   after a fatal system error. This pathname is shared by all hosts, 
#   but the file must be unique on each host so this must reference a 
#   local file system.
#
# Example:
# 
# SlurmdSpoolDir=/var/tmp/slurmd	# default: "/var/spool/slurmd"

SlurmdSpoolDir=/var/run/slurm-llnl/slurmd

#
# o Define the "slurm" user
#
#   "SlurmUser" specifies the user that the SLURM controller should run
#   as. The slurm controller has no need to run with elevated privileges,
#   so a user other than "root" is suggested here. 
#
# Example:
#
# SlurmUser=slurm	# default: "root"

SlurmUser=slurm

#
# o If you have a slow NIS environment,
#
#   big parallel jobs take a long time to start up (and may eventually
#   time-out) because the NIS server(s) may not be able to quickly
#   respond to simultaneous requests from multiple slurmd's.  You can
#   instruct slurmd to cache /etc/groups entries to prevent this from
#   happening by setting "CacheGroups=1".  Reconfiguring ("scontrol reconfig") 
#   with CacheGroups=0  will cause slurmd to purge the cache.
#
#   WARNING: The group ID cache does not try to keep itself in sync with
#            the system.  You MUST run "scontrol reconfig" to update the
#            cache after making any changes to system password or group
#            databases.
#
# Example:
#
# CacheGroups=1		# default is `0'


#
# o Define the slurmctld and slurmd server port numbers
#
#  by default, the slurmctld ports are set by checking for an entry in
#  /etc/services, and if that fails, by using an internal default set
#  at build time. That process will be overridden by these config 
#  parameters.
#
#    "SlurmctldPort"    : slurmctld server port 
#    "SlurmdPort"       : slurmd server port
#
# Example:
#
# SlurmctldPort=7010 	# default is `6817'
# SlurmdPort=7011       # default is `6818'


#
# o Define an alternate location for slurmd and slurmctld pid files, 
#   SlurmctldPidFile and SlurmdPidFile should have different values
#  
#    "SlurmctldPidFile" : fully qualified pathname containing slurmctld pid
#
#    "SlurmdPidFile"    : fully qualified pathname containing slurmd pid
#    
# Example:
#
# SlurmctldPidFile=/var/slurm/slurmctld.pid  # default: "/var/run/slurmctld.pid"
# SlurmdPidFile=/var/slurm/slurmd.pid        # default: "/var/run/slurmd.pid"

SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid

#
# o Define the authentication method for communicating between SLURM
#   components
#
# "auth/none"   : no authentication, the default
# "auth/authd"  : Brent Chun's authd
# "auth/munge"  : LLNL's munge
#
# WARNING: The use of "auth/none" permits any user to execute jobs as any 
# other user. This may be fine for testing purposes, but do not use it in
# production.
#
# Example:
#
# AuthType=auth/munge	# default: "auth/none"

AuthType=auth/munge

#
# o Define the switch or interconnect in use.
#
# "SwitchType"         : the type of switch or interconnect.
#     switch/none      : the default, supports all switches not requiring
#                        special set-up for job launch including Myrinet, 
#                        Ethernet, and InfiniBand.
#     switch/federation: IBM Federation switch
#     switch/elan      : Quadrics Elan 3 or Elan 4 interconnect.
#
# Example:
#
# SwitchType=switch/elan	# default: "switch/none"


#
# o Define the process tracking mechanism in use.
#
# "ProctrackType"           : the type of process tracking mechanism
#     "proctrack/aix"       : use AIX kernel extension for process tracking,
#                             the default value on AIX computers
#     "proctrack/linuxproc" : use parent process ID to establish process
#                             tree, required for MPICH-GM use
#     "proctrack/rms"       : use Quadrics kernal infrastructure to track 
#                             processes, strongly recommended for systems
#                             with a Quadrics switch
#     "proctrack/sgi_job    : which uses SGIâ€™s Process Aggregates (PAGG)
#                             kernel module, see http://oss.sgi.comm/projects/pagg/
#                             for more information
#     "proctrack/pgid"      : use Unix process group ID for process tracking,
#                             the default value on all other computers
# 
# Example:
#
# ProctrackType=proctrack/linuxproc	# default "proctrack/pgid"


#
# o Define the places to look for SLURM plugins.  This is a
#   colon-separated list of directories, just like the PATH
#   environment variable.
#
# Example:
#
# PluginDir=/etc/slurm/plugins:/usr/local/etc	# default: /usr/lib/slurm


#
# o Define other miscellaneous SLURM controller configuration values:
#
#    "FastSchedule"     : if set to `1' consider the configuration of nodes
#                         to be exactly that set in the config file, if set
#                         to `0' consider configuration of nodes to that
#                         which is reported by the node's slurmd, if set to
#                         `2' consider the configuration of each node to be
#                         that specified in the slurm.conf configuration
#                         file (as `1') and any node with less resources
#                         than configured will not be set DOWN.
#                         This can be useful for testing purposes.
#                         A FastSchedule value of zero will result in
#                         significantly slower scheduling.
#
#    "FirstJobId"       : Number of the first assigned job id.
#
#    "ReturnToService"  : if set to `1,' nodes in the DOWN state will be
#                         set to IDLE after they come back up. Otherwise,
#                         nodes will stay in the down state until manually
#                         brought into the IDLE state.
# 
#    "MaxJobCount"      : defines the maximum number of jobs slurmctld can 
#                         have in its active database at one time. Set the 
#                         values of MaxJobCount and MinJobAge so as to avoid 
#                         having slurmctld exhaust its memory or other
#                         resources.
#
#    "MpiDefault"	: define the default type of MPI to be used. If
#			  srun does not specify another value, slurm will 
#			  establish the environment for this mpi to execute.
#			  Currently supported values are lam (for LAM MPI and 
#                         Open MPI), mpich-gm, mvapich, and none (default, 
#                         which works for most other versions of MPI).
#
# Example:
#
# FastSchedule=0		# default is `1'
# FirstJobid=1000       	# default is `1'
# ReturnToService=1     	# default is `0'
# MaxJobCount=10000		# Defaults to 2000
# MpiDefault			# default is "none"

ReturnToService=1

#
# o Define Process Priority Propagation Configuration
#
#    "PropagatePrioProcess"
#                       : if set to `1', the priority (aka nice value) of the
#                         process that launched the job on the submit node,
#                         (typically the users shell), will be propagated to
#                         the compute nodes and set for the users job.  If set
#                         to `0', or left unset, the users job will inherit the
#                         scheduling priority from the slurm daemon.
#
# Example:
#
# PropagatePrioProcess=1       # default is `0'


#
# o Define the Resource Limit Propagation Configuration
#
#   These two parameters can be used to specify which resource limits to
#   propagate from the users environment on the submit node to the users job
#   environment on the compute nodes.  This can be useful when system limits
#   vary among nodes.  By default, (when neither parameter is  specified), all
#   resource limits are propagated.   The values of non-propagated resource
#   limits are determined by the system limits configured on the compute
#   nodes.   Only one of these two parameters may be specified.
#
#    "PropagateResourceLimits"       : A list of one or more comma-separated
#                                      resource limits whose (soft) values
#                                      will be set at job startup on behalf of
#                                      the user.  Any resource limit that is
#                                      not listed here, will not be propagated,
#                                      (unless the user overrides this setting
#                                      with the 'srun --propagate' switch).
#
#
#    "PropagateResourceLimitsExcept" : A list of one or more comma-separated
#                                      resource limits which will not be
#                                      propagated.  Any resource limit that is
#                                      not listed here, will be propagated.
#   
#                                The following resource limits are supported:
#
#                                RLIMIT_NPROC   RLIMIT_MEMLOCK   RLIMIT_CORE
#                                RLIMIT_FSIZE   RLIMIT_CPU       RLIMIT_DATA
#                                RLIMIT_STACK   RLIMIT_RSS       RLIMIT_NOFILE
#                                RLIMIT_AS
#
# Examples:
#
# PropagateResourceLimits=RLIMIT_CORE,RLIMIT_DATA # The users RLIMIT_CORE and
#                                                 # RLIMIT_DATA resource limit
#                                                 # soft values will be applied
#                                                 # to the job on startup.  All
#                                                 # other resource limit soft
#                                                 # values are determined by the
#                                                 # system limits defined on
#                                                 # the compute nodes.
#
# PropagateResourceLimitsExcept=RLIMIT_MEMLOCK    # All limits, except for
#                                                 # MEMLOCK, are propagated.
#


#
# o Define whether PAM (Pluggable Authentication Modules for Linux) will be
#   used.
#
# PAM is a set of shared libraries that enables system administrators to select
# the mechanism individual applications use to authenticate users. PAM also
# provides services for account managment, credential management, session 
# management and authentication-token (password changing) management. SLURM
# uses PAM to obtain resource limits. This allows the system adminisrator to
# dynamically configure resource limits without causing an interruption to
# the service provided by SLURM.
#
# Also, for PAM to work properly with SLURM, a configuration file for SLURM
# must be created and installed. See the slurm.conf man page for details about
# this file.
#
# Example:
#
# UsePAM=1 or UsePAM=Yes   # default is not to use PAM


#
# o Define an epilog and a prolog
#
#    "Prolog" : fully qualified path to script that will be executed as 
#               root on every node of a user's job before the job's tasks
#               will be initiated there.
#
#    "Epilog" : fully qualified path to a script that will be executed as
#               root on every node of a user's job after that job has 
#               terminated.
#
# Example:
#
# Prolog=/usr/local/slurm/prolog	# default is no prolog
# Epilog=/usr/local/slurm/epilog	# default is no epilog


# 
# o Define programs to be executed by srun at job step initiation and 
#   termination. These parameters may be overridden by srun's --prolog 
#   and --epilog options.
#
# Example:
#
# SrunProlog=/usr/local/slurm/srun_prolog   # default is no srun prolog
# SrunEpilog=/usr/local/slurm/srun_epilog   # default is no srun epilog


#
# o Define task launch specific parameters
#
#    "TaskProlog" : Define a program to be executed as the user before each 
#                   task begins execution.
#    "TaskEpilog" : Define a program to be executed as the user after each 
#                   task terminates.
#    "TaskPlugin" : Define a task launch plugin. This may be used to 
#                   provide resource management within a node (e.g. pinning
#                   tasks to specific processors). Permissible values are:
#      "task/none"     : no task launch actions, the default.
#      "task/affinity" : CPU affinity support (see srun man pages for 
#                        --cpu_bind and --mem_bind options)
#
# Example:
#
# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none
# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none
# TaskPlugin=task/affinity                    # default is task/none


#
# o Define the temporary file system 
#
#    "TmpFS"  : Defines the location of local temporary storage filesystem 
#               on remote nodes. This filesystem will be used in reporting
#               each node's TmpDisk space.
#
# Example:
#
# TmpFs=/var/tmp	# default "/tmp"


#
# o Define the location of the private and public keys used by SLURM
#   to generate job credentials.
#
#    "JobCredentialPrivateKey"       : Full pathname to the private key
#
#    "JobCredentialPublicCertificate : Full pathname to the public cert.
#
# Example:
#
# JobCredentialPrivateKey=/etc/slurm/slurm.key
# JobCredentialPublicCertificate=/etc/slurm/slurm.cert 

JobCredentialPrivateKey=/etc/slurm-llnl/slurm.key
JobCredentialPublicCertificate=/etc/slurm-llnl/slurm.cert

#  "CheckpointType" : Define the system-initiated checkpoint method to be
#                     used for user jobs. Acceptable values at present
#		     include "checkpoint/aix" (only on AIX systems),
#		     "checkpoint/ompi" (requires OpenMPI version 1.3 or
#		     higher), "checkpoint/xlch" (for XLCH, requires that
#		     SlurmUser be root), and "checkpoint/none". The default
#		     value is "checkpoint/none".
# Example:
#
# CheckpointType=checkpoint/ompi	# default: "checkpoint/none"

# "CryptoType" : Define the cryptographic signature tool to be used in the
#                creation of job step credentials. Acceptable values at present
#                include "crypto/munge" and "crypto/openssl"
#
# Example:
#
# CryptoType=crypto/munge 	# default: "crypto/openssl"

CryptoType=crypto/openssl

# "DisableRootJobs" : If set to 1 then user root will be prevented from running
#                     any jobs.
# Example:
#
# DisableRootJobs=1	# default: `0'

# "JobFileAppend" : This option controls what to do if a jobâ€™s output or error
#                   file exist when the job is started. If JobFileAppend is set
#                   to a value of 1, then append to the existing file.
# Example:
#
# JobFileAppend=1 	# default:`0'

# "JobRequeue" : This option controls what to do by default after a node failure.
#                If JobRequeue is set to a value of 1, then any job running on
#                the failed node will be requeued for execution on different
#                nodes. If JobRequeue is set to a value of 0, then any job 
#                running on the failed node will be terminated.
# Example:
#
# JobRequeue=1		# default: `0'

# "MailProg" : Fully qualified pathname to the program used to send email per
#              user request.
# Example:
#
# MailProg=/usr/bin/mail	# default: "/bin/mail"

MailProg=/usr/bin/mail

# "PrivateData" : If non-zero then users are unable to view jobs or job steps
#                 belonging to other users (except for SlurmUser or root, who can
#                 view all jobs). Otherwise any user is permitted
#                 to view any jobs or job steps.
# Example:
#
# PrivateData=1		# default: `0'


#
#     Node and Partition Configuration
#     ========================================================================

#
#  o Node configuration
#
#    The configuration information of nodes (or machines) to be managed 
#    by SLURM is described here. The only required value in this section
#    of the config file is the "NodeName" field, which specifies the 
#    hostnames of the node or nodes to manage. It is recommended, however,
#    that baseline values for the node configuration be established
#    using the following parameters (see slurm.config(5) for more info): 
#
#     "NodeName"   : The only required node configuration parameter, NodeName
#                    specifies a node or set of nodes to be managed by SLURM.
#                    The special NodeName of "DEFAULT" may be used to establish
#                    default node configuration parameters for subsequent node
#                    records. Typically this would be the string that 
#                    `/bin/hostname -s` would return on the node. However 
#                    NodeName may be an arbitrary string if NodeHostname is 
#                    used (see below).
#
#     "Feature"    : comma separated list of "features" for the given node(s) 
#
#     "NodeAddr"   : preferred address for contacting the node. This may be 
#                    either a name or IP address.
#
#     "NodeHostname"
#                  : the string that `/bin/hostname -s` would return on the
#                    node.  In other words, NodeName may be the name other than
#                    the real hostname.
#
#     "RealMemory" : Amount of real memory (in Megabytes)
#
#     "Procs"      : Number of CPUs 
#
#     "State"      : Initial state (IDLE, DOWN, etc.)
#
#     "TmpDisk"    : Temporary disk space available on node
#
#     "Weight"     : Priority of node for scheduling purposes
#
#   If any of the above values are set for a node or group of nodes, and
#   that node checks in to the slurm controller with less than the 
#   configured resources, the node's state will be set to DOWN, in order
#   to avoid scheduling any jobs on a possibly misconfigured machine.
#
# Example Node configuration:
#
# NodeName=DEFAULT Procs=2 TmpDisk=64000 State=UNKNOWN
# NodeName=host[0-25] NodeAddr=ehost[0-25] Weight=16
# NodeName=host26     NodeAddr=ehost26     Weight=32 Feature=graphics_card

NodeName=server Procs=1 State=UNKNOWN

#
# o Partition Configuration
#
#   Paritions are groups of nodes which (possibly) have different limits
#   and access controls. Nodes may be in multiple partitions. Jobs will
#   not be allowed to span partitions. The following partition configuration
#   parameters are recognized:
#
#    "PartitionName" : Name used to reference this partition. The special
#                      PartitionName of "DEFAULT" may be used to establish
#                      default partition configurations parameters for 
#                      subsequent partition records.
#
#    "Nodes"         : list of nodes that compose this partition
#
#    "AllowGroups"   : Comma separated list of group ids which are allowed
#                      to use the partition. Default is "ALL" which allows
#                      all users to access the partition.
#
#    "Default"       : if "YES" the corresponding partition will be the 
#                      default when users submit jobs without specification
#                      of a desired partition.
#
#    "RootOnly"      : only user id zero (root) may use this partition
#
#    "MaxNodes"      : Maximum count of nodes that will be allocated to any
#                      single job. The default is unlimited or `-1'
#
#    "MaxTime"       : Maximum timelimit of jobs in this partition in minutes.
#                      The default is unlimited or `-1'
#
#    "MinNodes"      : Minimum count of nodes that will be allocated to any
#                      single job. The default is `1'
#
#    "Shared"        : Allow sharing of nodes by jobs. Possible values are
#                      "YES" "NO" or "FORCE"
#
#    "State"         : State of partition. Possible values are "UP" or "DOWN"
#
#
# Example Partition Configurations:
#
# PartitionName=DEFAULT MaxTime=30 MaxNodes=26
# PartitionName=debug Nodes=host[0-8,18-25] State=UP Default=YES
# PartitionName=batch Nodes=host[9-17,26]   State=UP

PartitionName=debug Nodes=server Default=YES MaxTime=INFINITE State=UP

#
#     Timers Section
#     ========================================================================

#
#    "SlurmctldTimeout" : amount of time, in seconds, backup controller
#                         waits for primary controller to respond 
#                         before assuming control.
#
#    "SlurmdTimeout"    : amount of time, in seconds, the controller
#                         waits for slurmd to respond before setting the
#                         node's state to DOWN. If set to 0, this feature
#                         is disabled.
#
#    "InactiveLimit"    : The interval, in seconds, a job or job step is 
#                         permitted to be inactive (srun command not responding)
#                         before being terminated.
#
#    "MinJobAge"        : The time, in seconds, after a job completes before
#                         its record is purged from the active slurmctld data.
#   
#    "KillWait"         : The time, in seconds, between SIGTERM and SIGKILL
#                         signals sent to a job upon reaching its timelimit.
#
#    "WaitTime"         : Specifies how many seconds srun should wait after the 
#                         first task terminates before terminating all remaining  
#                         tasks. If set to 0, this feature is disabled.
#
#    "EpilogMsgTime"    : The  number of microseconds the the slurmctld daemon
#                         requires to process an epilog completion message from
#                         the  slurmd  dameons.
#    "MessageTimeout"   : Time permitted for a round-trip communication to
#                         complete in seconds.
#
# Example:
#
# SlurmctldTimeout=120	# Defaults to 120 seconds
# SlurmdTimeout=300	# Defaults to 300 seconds
# InactiveLimit=600	# Defaults to 0 (unlimited)
# MinJobAge=30		# Defaults to 300 seconds
# KillWait=10		# Defaults to 30 seconds
# WaitTime=30		# Defaults to 0 (unlimited)
# EpilogMsgTime=2000	# Defaults to 2000 microseconds
# MessageTimeout=10     # Defaults to 10 seconds

SlurmctldTimeout=300

#
# "UnkillableStepTimeout" : The length of time, in seconds, that SLURM will wait
#                           before deciding that processes in a job step are
#                           unkillable (after they have been signaled with
#                           SIGKILL).
#                           The default timeout value is 60 seconds.
# "UnkillableStepProgram" : If the processes in a job step are determined to be
#                           unkillable for a period of time specified by the
#                           UnkillableStepTimeout variable, the program
#                           specified by the UnkillableStepProgram string will
#                           be executed.
#
# Example:
#
#UnkillableStepProgram=
#UnkillableStepTimeout=60

#
#    "HealthCheckInterval" : The interval in seconds between executions of
#                            HealthCheckProgram. The default value is zero,
#                            which disables execution.
#    "HealthCheckProgram" :  Fully qualified pathname of a script to execute
#                            as user root periodically on all compute nodes
#                            that are not in the DOWN state.
#
# Example:
#
#HealthCheckInterval=0   # Default to 0 (disabled)
#HealthCheckProgram=     # By Default, no program will be executed



#
#     Scheduling Section
#     ========================================================================

#
#
# o Define a scheduler.
#
# "SchedulerType"	 : the type of scheduler. Orders pending jobs.
#	"sched/builtin"	 : the default, SLURM's built-in FIFO scheduler.
#	"sched/backfill" : FIFO scheduling with backfill.
#	"sched/hold"     : hold all new jobs if "/etc/slurm.hold" exists, 
#	                   otherwise perform FIFO scheduling.
#	"sched/wiki"	 : the Wiki interface to Maui.
#	"sched/gang"     : for gang scheduler (time-slicing of parallel jobs)
#
# "SchedulerPort"	 : for polling schedulers, the port number on
#			   which slurmctld should listen for connection
#			   requests.
#
# "SchedulerRootFilter"	 : for schedulers that support it (currently only
#			   sched/backfill). If set to '1' then scheduler
#			   will filter and avoid RootOnly partitions (let
#			   root user or process schedule these partitions).
#			   Otherwise scheduler will treat RootOnly
#			   partitions as any other standard partition.
#
# Example:
#
# SchedulerType=sched/wiki	# default: "sched/builtin"
# SchedulerAuth=42
# SchedulerPort=7321
# SchedulerRootFilter=0	# default is '1'

SchedulerType=sched/backfill

# "SchedulerTimeSlice" : Number of seconds in each time slice when
#                        SchedulerType=sched/gang.
# Example:
#
# SchedulerTimeSlice=15	# default: `30'

# "DefMemPerTask" : Default real memory size available per task in MegaBytes.
# Example:
#
# DefMemPerTask=512	# default: `0' (unlimited).

# "MaxMemPerTask" : Maximum real memory size available per task in MegaBytes.
# Example:
#
# MaxMemPerTask=512	# default: `0' (unlimited).

# "SelectType" : Identifies the type of resource selection algorithm to be
#                used. Acceptable values include "select/linear" for
#                sequentially ordered nodes in a one-dimensional array,
#                "select/cons_res" for resources within a node individually
#                allocated as consumable or "select/bluegene"
#                for a three-dimensional BlueGene system.
# Example:
#
# SelectType=select/cons_res		# default: "select/linear"


# "SelectTypeParameters" : Specify consumable resources. The only supported
#                          option  for SelectType=select/linear is CR_Memory,
#                          the other values are supported only for
#                          SelectType=select/cons_res:
#                          "CR_CPU"           : CPUs are consumable resources
#                          "CR_CPU_Memory"    : CPUs and memory are consumable
#                                               resources
#                          "CR_Core"          : Cores are consumable resources
#                          "CR_Core_Memory"   : Cores and memory are consumable
#                                               resources
#                          "CR_Socket"        : Sockets are consumable resources
#                          "CR_Socket_Memory" : Memory and CPUs are consumable
#                                               resources
#                          "CR_Memory"        : Memory is a consumable resource.
#
# Example:
#
# SelectTypeParameters=CR_CPU


#
#     Logging and Accounting 
#     ========================================================================

#
# o Define slurmd and slurmctld logging options
#
#    "SlurmctldDebug"   : verbosity of slurmctld log messages 
#                         (Values from 0 to 7 are legal, with `0' being
#                          "quiet" operation and `7' being insanely verbose)
#
#    "SlurmdDebug"      : verbosity of slurmd log messages (0-7, see above)
#
#    "SlurmctldLogFile" : fully qualified pathname to slurmctld logfile
#                         (If a logfile is set for slurmctld, logging via
#                          syslog will be turned off)
#
#    "SlurmdLogFile"    : fully qualified pathname to slurmd logfile,
#                         may contain "%h" for hostname substitution
#                         (same caveat as SlurmctldLogFile above)
#
# Example:
#
# SlurmctldDebug=4	# default is `3'  
# SlurmdDebug=4		# default is `3'
#

SlurmdLogFile=/var/log/slurm-llnl/slurmd.log
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log

# "AccountingStorageEnforce" : If set to a non-zero value and the user,
#                              partition, account association is not defined
#                              for a job in the accounting database then
#                              prevent the job from being executed.
# Example
#
# AccountingStorageEnforce=1	# default=`0'

# "AccountingStorageHost" : Define the name of the host where the database is
#                           running we are going to store the accounting data.
# "AccountingStorageType" : Define the accounting storage mechanism type.
#                           Acceptable values include:
#                           "accounting_storage/filetxt" for writing account
#                           records in a text file,
#                           "accounting_storage/gold" for using the Gold
#                           system,
#                           "accounting_storage/mysql" for using MySQL,
#                           "accounting_storage/pgsql" for using PostgreSQL,
#                           "accounting_storage/slurmdbd" for using the
#                           SLURM database daemon system and
#                           "accounting_storage/none" which means that
#                           records are not maintained.
# "AccountingStorageLoc"  : Specifies name of the database as the location of
#                           the file or database where accounting records are
#                           written.
# "AccountingStoragePort" : Define the port the database server is listening
#                           on where we are going to store the accounting data.
# "AccountingStorageUser" : Define the name of the user we are going to connect
#                           to the database with to store the accounting data.
# "AccountingStoragePass" : Define the password used to gain access to the
#                           database to store the accounting data. 
#
# Example:
#
# AccountingStorageType=accounting_storage/mysql
# 					default: "accounting_storage/none"
# AccountingStorageHost=localhost
# AccountingStoragePort=3306
# AccountingStorageLoc=slurmdb		# name of the database	
# AccountingStorageUser=slurm		# name of the user	
# AccountingStoragePass=maradona10	# password


#
# "JobAcctGatherType" : Define the job accounting mechanism type. Acceptable
#                       values include "jobacct_gather/aix" (for AIX operating
#                       system), "jobacct_gather/linux" (for Linux operating
#                       system) and "jobacct_gather/none"
# "JobAcctFrequency"  : Define the Frequency of the JobAcct poll thread
#
# Example:
#
# JobAcctFrequency=30
# JobAcctGatherType=jobacct_gather/linux	#default: "jobacct_gather/none


# "ClusterName" : The name by which this SLURM managed cluster is known for
#                 accounting purposes. This is needed distinguish between
#                 accounting data from multiple clusters being recorded in
#                 a single database.
# Example:
#
# ClusterName=maradona


# "JobCompHost" : Define the name of the host where the database is
#                 running and used to store the job completion data.
# "JobCompPort" : Define the port the database server is listening on
#                 where we are going to store the job completion data.
# "JobCompLoc"  : The interpretation of this value depends upon the
#                 logging mechanism specified by the JobCompType
#                 parameter either a filename or a database name.
# "JobCompUser" : Define the name of the user we are going to connect
#                 to the database with to store the job completion data.
# "JobCompPass" : Define the password used to gain access to the
#                 database to store the job completition data.
# "JobCompType" : Define the job completion logging mechanism type. Acceptable
#                 values include:
#                 "jobcomp/filetxt" for writing account records in a text file,
#                 "jobcomp/gold" for using the Gold system,
#                 "jobcomp/mysql" for using MySQL,
#                 "jobcomp/pgsql" for using PostgreSQL,
#                 "jobcomp/slurmdbd" for using the SLURM database daemon,
#                 "jobcomp/script" for executing a script and
#                 "jobcomp/none" which means that records are not maintained.
# Example:
# JobCompType=jobcomp/mysql 	# default: "jobcomp/none"
# JobCompHost=localhost
# JobCompPort=3306
# JobCompLoc=slurmjobcompdb	# name of the database	
# JobCompUser=slurm		# name of the user	
# JobCompPass=maradona10	# password


#
# o Define slurmd and slurmctld logging options
#
#    "SlurmctldDebug"   : verbosity of slurmctld log messages 
#                         (Values from 0 to 7 are legal, with `0' being
#                          "quiet" operation and `7' being insanely verbose)
#
#    "SlurmdDebug"      : verbosity of slurmd log messages (0-7, see above)
#
#    "SlurmctldLogFile" : fully qualified pathname to slurmctld logfile
#                         (If a logfile is set for slurmctld, logging via
#                          syslog will be turned off)
#
#    "SlurmdLogFile"    : fully qualified pathname to slurmd logfile,
#                         may contain "%h" for hostname substitution
#                         (same caveat as SlurmctldLogFile above)
#
# Example:
#
# SlurmctldDebug=4	# default is `3'  
# SlurmdDebug=4		# default is `3'
#
# SlurmctldLogFile=/var/log/slurmctld.log  # default is to log via syslog()
# SlurmdLogFile=/var/log/slurmd.log.%h     # substitute hostname for "%h"


#
#     Power save support for idle nodes (optional)
#     ========================================================================

# "SuspendProgram"  : The program that will be executed when a node remains
#                     idle for an extended period of time. This program is
#                     expected to place the node into some power save mode.
# "ResumeProgram"   : The program that will be executed when a node in power
#                     save mode is assigned work to perform.
# "ResumeRate"      : The rate at which nodes in power save mode are returned
#                     to normal operation by ResumeProgram.
# "SuspendExcNodes" : Specifies the nodes which are to not be placed in power
#                     save mode, even if the node remains idle for an
#                     extended period of time. 
# "SuspendExcParts" : Specifies the partitions whose nodes are to not be
#                     placed in power save mode, even if the node remains
#                     idle for an extended period of time.
# "SuspendRate"     : The rate at which nodes are place into power save mode
#                     by SuspendProgram.
# "SuspendTime"     : Nodes which remain idle for this number of seconds will
#                     be placed into power save mode by SuspendProgram
#
# Example:
# SuspendProgram=/usr/share/doc/slurm-llnl/examples/slurm-suspend.sh
# ResumeProgram=/usr/share/doc/slurm-llnl/examples/slurm-resume.sh
# ResumeRate=32
# SuspendExcNodes=nfsnode
# SuspendExcParts=severs
# SuspendRate=32
# SuspendTime=60
